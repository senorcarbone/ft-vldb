%!TEX root = main.tex
\begin{figure*}[t!]
\centering
\includegraphics[width=\textwidth]{figures/flinkoverview.pdf}
\caption{An Overview of the Apache Flink System Model and Architecture.} 
\label{fig:flink-overview}
\vspace{-4mm}
\end{figure*}

\vspace{-2mm}

\vspace{-2mm}
\section{Preliminaries}
\label{sec:preliminaries}


\subsection{The Apache Flink System}
The Apache Flink system \cite{CUSTOM:web/Flink} is an open-source project that provides a full software stack for programming, compiling and running distributed continuous data processing pipelines (\autoref{fig:flink-overview}(a)). Pipelines can be written as a series of data-centric transformations expressed in a fluid, functional programming API (in Scala, Java or Python) inspired by Flume Java\cite{chambers2010flumejava}, Dryad LINQ\cite{yu2008dryadlinq}, Naiad\cite{murray2013naiad} and based in its majority on Google's Dataflow Model \cite{akidau2015dataflow} (i.e., identical windowing semantics and out-of-order processing logic). At the core of the model there are two basic abstract data types, the \emph{DataSet} and \emph{DataStream} representations which target bounded and unbounded datasets respectively. Computation declared using the available higher-level domain-specific libraries such as the SQL and Machine Learning (ML) packages, translates into a logical pipeline using these core representations. A major distinctive trait of the Flink programming model compared to state of the art is the capability to declare local or partitioned, persistent application state within continuous user-defined transformations through managed data collections with diverse properties (append-only, mutable, etc.). Flink's runtime ensures that consistency is guaranteed for any managed state declared despite potential partial failures or reconfigurations, such as updates to the application code or changes in execution parallelism.

Logical pipeline representations are optimised at the client and shipped to Flink's runtime, a distributed, continuous dataflow execution environment. The runtime derives a physical deployment of tasks and manages their continuous execution as depicted in \autoref{fig:flink-overview}(b). As with most distributed data processing systems, Flink's runtime consists of a \emph{JobManager}, a master process that holds the metadata of active pipelines and coordinates execution by communicating with worker processes, the TaskManagers. Communication between the the JobManager and TaskManagers respects an asynchronous RPC-based communication protocol, consisting of periodic status updates (heartbeats) to the JobManager and scheduling requests back to the TaskManagers. In contrast to batch-centric job management \cite{zaharia2012discretized} which prioritizes reconfiguration and coordination, Flink employs a schedule-once, long-running allocation of tasks. However, the system is flexible to reconfigure pipelines to more or less workers and re-allocate application state on-demand. This approach minimizes management overhead while still allowing for further adaptation to hardware or software changes or partial failures that can potentially occur. Finally, pipeline deployments in Flink are highly available, thus, tolerating even master failures via leader election and passive failover in Zookeeper. All underlying mechanisms for state partitioning, snapshotting and maintenance are the main focus and covered thoroughly in this paper.


\subsection{The Global Snapshotting Problem}

Distributed systems are typically designed to hide concerns related to their distributed nature from the user, offering the view of a single entity. For a distributed compute system like Flink we often have to reason about the state of a  pipeline in production at any time during its execution. Referring to the complete distributed state of a computation as an atomic unit, is essential to correctly rollback its full execution to the point in time when that global state was captured. This is crucial when reconfiguration is required or a partial failure caused a violation of the correct execution of the pipeline. Generally, this approach is also known as rollback recovery \cite{elnozahy2002survey}. 

Distributed snapshotting \cite{chandy1985distributed} protocols enable rollback recovery by producing a correct, complete state replica of a distributed execution which can be used to restore the system to an earlier point in time. In principle, a distributed system is a set of processes connected via data channels, abstractly represented as a directed graph of nodes and edges respectively. At any time during continuous system execution the complete state is reflected in the nodes and edges of that graph (i.e., internal state of processes and in-transit events). A consistent snapshot should capture the complete state while respecting causal execution dependencies so that no computational state or data are lost. 

Existing snapshotting protocols are tailored to specific types of graphs and vary in terms of complexity and performance. For example, Chandy and Lamport's original protocol \cite{chandy1985distributed} is designed for strongly connected directed graphs (there exist a path between any two processes) and it is transparently pipelined together with the normal execution of the system through the use of special markers, without affecting its operation. On the other hand, the same approach relies on aggressively logging any records that are in transit in the duration of the protocol and it is incapable of terminating on weakly connected graphs. 

Weakly connected graphs are inherently relevant to distributed dataflow processing systems \cite{murray2013naiad,jacques2016consistent,millwheel,chambers2010flumejava,castro2013integrating}. Data records are typically inserted to the system through special \emph{source} vertices while exiting through special \emph{sink} vertices and cycles can optionally exist. Proposed protocols for snapshotting weakly connected dataflow graphs such as Naiad's two-phase commit \cite{murray2013naiad} and IBM Streams' multi-stage snapshotting halt the regular operation of the system to complete the snapshot and also end-up logging in-transit records unnecessarily. In our approach, described thoroughly in \autoref{sec:core}, we show how it is possible to naturally pipeline the snapshotting process in weakly connected graphs and capture minimal state, skipping in-transit records when possible, without halting the overall progress of the system.

