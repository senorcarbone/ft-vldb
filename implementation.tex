%!TEX root = main.tex


\section{Implementation and Usage}

With explicit managed state and consistent snapshotting at its core, Flink maintains a rich ecosystem of backends, connectors and other services that interplay seamlessly and benefit from the capabilities of its global state establishment mechanism. In this section, we summarize how each of these subsystems builds on-top of Flink's core architecture, while expanding it with asynchronous communication, delivery guarantees and external state querying support.

\label{sec:implementation}

\subsection{State Backend Support}

Managed state consistency is coordinated by Flink's snapshotting algorithm (\autoref{sec:snapshots}), however it is the responsibility of the state backends to define access to the state and how the a snapshot of the state governed by the parallel task is taken. Based on how the state of each particular operator is stored and accessed we can distinguish two main classes of state backends. 1) Backends that hold state of the operators locally and 2) backends that backends that store the state in an external system such as a database or a key/value store.

Local state backends can keep their state completely in-memory , out-of-core or often as a combination of these approaches as done by RocksDB for instance.

Depending on the overall expected managed state accumulated in a pipeline, each of the backends offers a suitable trade-off between throughput and scalability.

\stephan{I honestly find the distinction between the In-Memory and File-based state backends confusing. Both hold working state in memory, and both write state to file systems on checkpoints (at least in HA mode) - the difference is to minor for a section in this paper. I also don't think that details about what state backends require additional serialization and which does not is relevant to conceptually understand the role of state backends and how the dataflow snapshots and the operator snapshots interact.}

\stephan{Suggestion: We can go even broader here: I would describe that the responsibility of the state backend is to define the access to the state and how a snapshot of the state governed by the parallel task is taken. That gives us conceptually two types of state backends: (1) State backends that hold stat in-process in some map, and (2) state backends that interact with external databases or k/v stores. For (1) We have a state backend that uses hash maps (in memory) or RocksDB (out of core). For (2), we can again describe two implementations, (a) transactionally and (b) with MVCC (multi-version concurrency control). For (a) it works as follows: Each checkpoint is one distributed bulk 2-phase commit transaction: during a checkpoint, changes are put into the transaction log, on checkpoint they are pre-committed, and when the checkpoint as a whole is complete, the transaction is full-committed. The fact that practically most databases do not expose the fine grained hooks to log, pre-commit, and full commit can be worked around by maintaining an own write-ahead log and triggering bulk-load of the write-ahead log as a transaction upon completed checkpoints. (b) MVCC is simpler, the version of each change is that of the checkpoint epoch, and a commit bumps the version, while a failed checkpoint drops the current version (its harder in practice, because again most MVCC databases lack the hooks, but its a nice way to think about the problem).
Let me know what you think! There is a bit of overlap with the "Consistent Output Commit" section.}

\para{In-Memory}
The \emph{In-Memory} backend maintains active managed state in local, allocated heap space and snapshotted state within the Job Manager's heap space. This is, in some distinct use-cases, a preferable choice that yields very high throughput. For example, pipeline testing and debugging or actual deployments with low state capacity requirements such as filtering and basic ETL can make a reasonable use of this backend. Upon each task's \texttt{triggerSnapshot()} invocation local state is serialized, copied and transfered to the Job Manager together with typical snapshot metadata. As a result, the Job Manager needs to have enough heap space allocated in order to be able sustain all physical task materialized states for each snapshot.

\para{File-Based:}
Most often, complete snapshots involve more aggregated state than the allocated memory of a single node. The \emph{File-Based} backend deviates from \emph{In-memory} only when it comes to snapshotting. Once a snapshot is invoked the state is being copied to a configured distributed file system directory (e.g., HDFS). As a result, snapshot metadata is also kept to a minimum, while throughput can remain high since operations on uncommitted state occur in-place at the local heap.

\para{Out-of-Core:}
Complex pipelines often need to maintain very large managed state such as long sliding windows. In such cases, local heap space is insufficient to sustain even locally accessed active state, especially for certain task operators (e.g., Flink's \texttt{WindowOperator}). Flink provides an \emph{out-of-core} backend that decouples state operations performed on each task from the physical location where state itself is persisted and updated. In its current implementation, out-of-core state is interfaced with an embedded file-backed key-value database (i.e., RocksDB). Therefore, state capacity is only limited by the file system space allocated at the host where this backend is deployed. Moreover, operations triggered on managed state such as \texttt{update} and \texttt{add} are carried over to the embedded key-value store instead. One of the key benefits, enabled out-of-the-box with such a scheme is that state updates can be executed asynchronously at the backend to maximize throughput. However, the main trade-off of accessing state off-heap is the extensive serialization overhead on each state access which can lead to lower throughput compared to heap-based active state. Nevertheless, this can be amortized, yielding less overhead during garbage collection (on-heap), flexible data locality, and copy-on-write support. An alternative version of the in-memory backend that operates on serialized data is currently considered to exploit the same benefits.

%\stefan{One big difference to the in-memory backends, and also a big source% of performance loss, is that this backend has to use serialization on each state read or update! Apart from this backend, keeping data serialized also has some advantages over objects on the heap: less overhead for the garbage collector, potentially higher data locality, and easier supoort for copy-on-write. This is why we are also planning to have an in-memory backend that works on serialized data.}. On the other hand, state value retrievals can take more time to complete, especially if some specific state has to be retrieved from compacted files on disk and it is no longer within the backend's memory.

\subsection{Asynchronous and Incremental Snapshots}
\label{sec:async}
One of the advantages of the pipelined snapshotting protocol presented in \autoref{sec:snapshots} is that it does not restrain the actual acquisition of snapshots to be synchronous. A call of \texttt{triggerSnapshot()} by each task is expected to create an identical copy of the current state of that task. However, the copy is not required to be a physical copy, it can be a \emph{logical snapshot} that is \emph{lazily materialized} in an asynchronous thread. This type of operation is supported well by many copy-on-write data structures. Concretely, we implemented the following techniques in Flink:

%One of the advantages of the pipelined snapshotting protocol presented in \autoref{sec:snapshots} is that it does not restrain the actual acquisition of snapshots to be synchronous. A call of \texttt{triggerSnapshot()} by each task is expected to create an identical copy of the current state of that task. In case there is support by a backend module to execute this operation asynchronously, it can be used without violating consistency.

The state backend based on RocksDB exploits the fact that RocksDB internally stores data in the form of an LSM (log-structured merge) tree [https://www.facebook.com/notes/facebook-engineering/under-the-hood-building-and-open-sourcing-rocksdb/10151822347683920/] - updates are not made in-place, but are appended and compacted \gyula{do we need more about LSM trees?}. When taking a snapshot, the synchronous \texttt{triggerSnapshot()} call simply marks the current version, which prevents all state as of that version to be overwritten during compactions. After that, the operator can continue processing and can make modifications to the state. An asynchronous thread iterates over the marked version, materializes it to the snapshot store, and finally releases the snapshot so that future compactions can overwrite that state again. Furthermore, the LSM-based data structure also lends itself to incremental checkpoints, which write only the tables/levels \gyula{should we explain what a leve/table is?} to the snapshot store that changed since the last checkpoint.

Flink's in-memory state backend implementation is based on an open-addressing hash table. When taking a snapshot, it synchronously copies the table array and starts the background materialization of the snapshot. The operator's regular stream processing thread lazily copies the state entries and overflow chains upon modification, if the materialization thread still holds onto the snapshot. While the in-memory state backend does not implement incremental checkpointing at this point, it is conceptually quite straightforward to add this using techniques like delta maps in the future.


%The \emph{out-of-core} backend of Flink offers the capability to trigger snapshots in a purely asynchronous way by copying all key-value states belonging to the current snapshot to a backup directory concurrently by another thread, thus, letting normal processing proceed without interruptions. \TODO{In RocksDB, for example, this is done by iterating exclusively through the RocksDB snapshot created at the time of the checkpoint. }
%\stefan{Right now I am also working on an asynchronous in-memory state backend. For the future we are planning to also allow for incremental snapshots of the RocksDB backend. If you want more information about this, let me know.} 
Once local snapshotting operations have been completed the notifications are triggered back at the tasks and carried out to the \emph{JobManager} alongside associated meta-data where a full snapshot can be declared as complete. Asynchronous snapshotting also unlocks the possibility to have multiple instances of the protocol running at the same time in a dataflow graph. \gyula{I dont think asynch snapshots have much to do with this really.} That means that while for example a periodic snapshot is undergoing, a user can also trigger a savepoint and both of the associated snapshots will run concurrently and distributively, each invoking its own asynchronous state copy and epoch.

Another feature that is provided to tasks as an optional asynchronous subscription-based mechanism is to trigger notifications about completed snapshots back at the tasks that request them. This is especially useful for garbage collection, discarding write ahead logs or for executing output commit protocols as we explain further in \autoref{sec:outputcommit}.

\subsection{Queryable State}

A recent addition among Flink's state management features is the ability to directly query managed state from outside the system. External systems can access Flink's keyed-state in a similar way as that of a key/value store, providing read-only access to the latest values computed by the stream processor. This feature is motivated by two observations. First, it is required by many applications to provide ad-hoc access to the application state. Secondly, that eager publishing of state to external systems frequently becomes a bottleneck in the application as remote writes to the external systems cannot keep up with the performance of Flink's local state on high-throughput streams.

Queryable state allows for any declared managed state within a pipeline, currently scoped by key, to be accessed outside the system for asynchronous reading, through a subscription-based API. First, managed state that allows for query access is declared in the original application. Upon state declaration, introduced in \autoref{sec:managedstate}, it is possible to allow access from external queries by simply setting a flag in the descriptor that is used to create the actual state, having an assigned unique name for this specific state to be accessed, as such:

\begin{lstlisting}[language=scala]
//stream processing application logic
val descriptor: ValueStateDescriptor[MySchema] = ...
descriptor.setQueryable("myKV")
...
val mutState: ValueState[MySchema] = ctx.getState(descriptor)
\end{lstlisting}

\para{} Upon deployment, a state registry service gets initiated and runs concurrently with the task that holds write access to that state. A client that wishes to read the state for a specific key can, at any time submit an asynchronous query (obtaining a \texttt{future}) to that service, specifying the job id, registered state name and key, as shown below:

\begin{lstlisting}[language=scala]
//client logic
val client = QueryableStateClient(cfg);
var readState: Future[_] = client.getKVState(job, "myKV", key);
\end{lstlisting}
%MyDeserializer[MySchema](Await.result(readState, timeout));

The current implementation of queryable state supports point lookups of values by key. The query client asks the Flink master (JobManager) for the location of the operator instance holding the state partition for the queried key. The client then sends a request to the respective TaskManager, which retrieves the value that is currently held for that key from the state backend. From a traditional database isolation-level viewpoint, the queries access \emph{uncommitted state}, thus following the \emph{read-uncommitted} isolation level. As future work, we plan to add \emph{read-committed} semantics by letting TaskManagers hold onto the state snapshots of committed checkpoints, and use those snapshots to answer state queries.

%compared to already provided \emph{read-committed} read isolation guaranteed by its specialized sinks through its snapshotting mechanism (see \autoref{sec:outputcommit}).

\subsection{Consistent Output Commit}
\label{sec:outputcommit}

\stephan{How about 'Exactly-once sinks'?}

\stephan{I would suggest to also change this section to follow a more conceptual view. There are two types of sinks (1) Transactional sinks and (2) idempotent sinks (and (2.1) exactly-once state can turn non-idempotent sinks into idempotent sinks).}

\stephan{The transactional sinks come in two flavors: (1) Works like 2-phase commit: on shapshot (= pre-commit) they ensure persistence of their data and on "notify checkpoint complete" they commit. Example of that variant is a transactional SQL sink (possibly realized with a Flink-managed write-ahead-log), or any write-ahead-ligging sink. Sub-flavor (2) is always writing and rolling back on failures, exemplified by the rolling file sink. On snapshot (= pre-commit) ensures persistence (flush / sync to filesystem) and remember their version, on failure rollback (truncate file)}

\stephan{For idempotent sinks: When a sink operation is truly \emph{idempotent}, no need to do anything (by definition). As an optimization, to avoid "read uncommitted", one can write changes to a write-ahead log and publish only when the checkpoint is completed (which makes it actually a transactional sink). When updates by key are not idempotent, one can make them idempotent by having a copy of the state in Flink's managed state, apply the changes to the Flink-managed state (which is exactly once) and pushing out the new values from that state (either directly or on checkpoint complete).}

So far we have considered consistency guarantees associated with the internal state of the system. However, it is most often important to offer guarantees regarding the side effects that a pipeline leaves to the outside world, whether that is a distributed database, file system or message queue. A pipeline interfaces with the outside world mainly via its dataflow sinks. Thus, it is often crucial that sinks can offer exactly-once delivery guarantees. The feasibility of achieving ``read-committed'' isolation guarantees to external writes depends on the properties of the system upon which sinks commit output and typically comes at a higher latency cost that can, at-times, violate strong SLAs on latency. A pipeline can always be halted between snapshots after a failure or an urgent reconfiguration request and both input and state can be rolled back consistently, as it was described in \ref{sec:core}. However, the same cannot always be guaranteed about the output. If sinks are connected, for example, to a printer that instantly flushes data on paper, a rollback would possibly print the same or alternating text twice. Flink's programming model is equipped with two main types of sinks that facilitate ``read-committed'' output and build on the snapshotting mechanism: the 1) Idempotent Database Sink and 2) Bucketing File Sink.

\para{Idempotent Database Sink: } Idempotency is a property used extensively by several systems at the presence of failures in order to encourage repeatability and alleviate bookkeeping efforts and complex decision protocols to offer delivery guarantees\cite{CUSTOM:web/SparkStructuredStreaming,millwheel}. Flink's database sink executes user-defined, idempotent database queries to a distributed database for each input received per parallel sink. Deterministic streams (that do not involve stream interleaving or other forms of non-determinism) can rely on that sink to operate consistently with the database. However, in most cases where deterministic processing cannot be guaranteed, a write-ahead log of prepared query statements is kept as part of the state of that sink and maintained per-epoch. Once an asynchronous notification (\autoref{sec:async}) arrives regarding a completed epoch snapshot, the database sink commits all pending writes to the database at the expense of additional output latency (for a snapshot to complete and its notification to arrive to the sinks). Query idempotency guarantees that even failures during committing can be resolved by simply re-committing the same queries and thus, eventually, leaving the same side effects upon subsequent system reconfigurations. 
%\paris{what are the database properties required and why is it only cassandra based?}

\begin{figure}[t!]
\centering
\includegraphics[width=\textwidth / 2]{figures/filecommit.pdf}
\caption{A visualization of Bucketing File Sinks.} 
\label{fig:filecommit}
\vspace{-4mm}
\end{figure}

\para{Bucketing File Sink: } Committing state to a distributed file systems (e.g. HDFS) has to be made in a coordinated way since a file has to be consistent across all its file partitions. Flink's bucketing file sink (depicted in \autoref{fig:filecommit}) eliminates the need for a write-ahead log and eagerly appends stream output within uncommitted distributed file directories which group (or ``bucket'') partition parts by time-period. After a pre-configured inactivity time-period \texttt{in-progress} directories become \texttt{pending} and are ready to be committed. The Bucketing File Sink integrates with Flink's snapshotting algorithm and associates epochs with buckets. Once a file bucket is under \texttt{pending} mode and an asynchronous notification for an associated epoch has been received, it can be moved to a \texttt{committed} state via a \texttt{rename} operation. Potential disruptions between epochs resolve into a \texttt{truncate} (Posix) operation, which is currently supported by major distributed file systems and conveniently reverses append operations. \paris{i guess the non-truncate approach is ugly. Should we write about it?}

%\subsection{Asynchronous IO}
%\paris{What I write here does not seem to be the actual implementation. The implementation does event sourcing which is a bit sucky tbh...}
%Most often, all logic within a pipeline is executed within its stateful tasks and data ingestion from external systems flows directly through pipeline sources. Flink enforces this restriction in order to guarantee consistent, managed state and more importantly to avoid \emph{blocking} operations that can create a bottleneck or even a deadlock in a pipeline. If the application logic deems access to external systems necessary, Flink offers an Asynchronous I/O model for such operations. 

%Special \texttt{AsyncIO} operators in Flink, provide the necessary callbacks for invoking and handling asynchronous, user-defined requests to external systems (e.g., a database). Asynchronous requests are internally logged and maintained as part of the managed state of \texttt{AsyncIO} operators. Responses are also logged in-advance as part of the state and their handling can be optionally pre-configured to be FIFO according to their invocation or event time order (based on low watermarks \cite{millwheel,akidau2015dataflow,li2008out}). Asynchronous operations integrate seamlessly with the core snapshotting algorithm since both invocation and handling logic is executed within the same regular processing thread of the encapsulating task, thus, yielding atomic and controlled access to managed state. In case of rollback recovery, all logged requests are being repeated and pending results are eventually processed within their respective epoch in the pre-defined order. There are no guarantees, whatsoever, regarding the side effects of potential async write operations that will be repeated in case or rollback recovery on external systems. Similarly to Google's Millwheel \cite{millwheel},  all async I/O operations that communicate with external systems should be idempotent to satisfy strong consistency guarantees on those systems.

\subsection{High Availability \& Reconfiguration}

All metadata associated with the active state of long-running pipelines is kept within the \texttt{JobManager} node. This makes the importance of this central node quite important for the general functionality of the system and evidently it is indeed a single-point-of-failure. A \texttt{JobManager} failure would halt the coordination of the snapshotting protocol, the collection and management of snapshot metadata as well as making further job deployments and rescaling requests unavailable, thus, eliminating the purpose of any fault tolerance mechanism. To deal with such a critical failure we employ a passive-standby high availability scheme where configured standby nodes can take undertake the coordination role of the failed master node via distributed leader election. When this mode is enabled, the coordination of vital decisions such as job deployment and scheduling are undertaken via a distributed decision protocol which logs committed operations atomically (currently utilizing a \texttt{Zookeeper} quorum executing the \texttt{zab} protocol \cite{hunt2010zookeeper}). This introduces some additional latency for such critical operations, however, non critical decisions such as the persistence of the most recent snapshot metadata for a job, are asynchronously committed and logged. The worst scenario of a potential failure during non-critical commits would simply result into, a yet valid restoration of active pipeline state from snapshots that correspond to earlier epochs that have been committed prior to the failure.

	